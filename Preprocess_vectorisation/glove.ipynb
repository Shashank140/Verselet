{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"glove.ipynb","provenance":[],"mount_file_id":"1kODjNEiZg6N8iEeVF6M19uOW_we2wqq1","authorship_tag":"ABX9TyNxkI6OFy8pIpSZyLCSXWxo"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Pe3tdpVVrsn","executionInfo":{"elapsed":20963,"status":"ok","timestamp":1635962202607,"user":{"displayName":"201951171 VISHAL SINGH RAJPUT","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11556631215676760612"},"user_tz":-330},"outputId":"a050a7d8-1ba4-4f3a-b49c-2ba73d5e0247"},"source":["# Mounting Google Drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sqQ2r6JsVyM5","executionInfo":{"elapsed":497855,"status":"ok","timestamp":1635962722541,"user":{"displayName":"201951171 VISHAL SINGH RAJPUT","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11556631215676760612"},"user_tz":-330},"outputId":"f5e4bc10-9e28-4a56-9e66-2a6ee1ea4c02"},"source":["import pickle\n","\n","# Initializing global variables\n","co_occurence_matrix = {}\n","vocabulary = []\n","local_context_window = 6\n","print(\"Global variables initialized...!!\")\n","\n","with open(\"/content/drive/MyDrive/Design_Project_Verselet/pre_processing+vectoristation/tf-idf/train_data_bow\",\"rb\") as f:\n","    corpus = pickle.load(f)\n","\n","training_data = []\n","\n","no_of_sentences_so_far = 0 \n","\n","# For all poems in the corpus, building the co-occurence matrix \n","for poem in corpus:\n","  poem = poem[-2]\n","\n","  no_of_sentences_so_far += 1\n","  if no_of_sentences_so_far%5000 == 0 :\n","    print(\"Co occurence matrix build in progress... \" + str(no_of_sentences_so_far) + \" poems read.\")\n","    \n","  for index_tokens_list in range(len(poem)):\n","    token = poem[index_tokens_list]\n","    if token not in vocabulary:\n","      vocabulary.append(token)  \n","\n","    # Iterating over all tokens and storing them in the co-occurence matrix\n","    for index in range(1, int(local_context_window/2)):\n","      if index + index_tokens_list >= len(poem):\n","        break\n","      tuple_main_context = (poem[index_tokens_list], poem[index_tokens_list + index])\n","      tuple_context_main = (poem[index_tokens_list + index], poem[index_tokens_list])\n","      if tuple_main_context not in co_occurence_matrix:\n","        co_occurence_matrix[tuple_main_context] = 1\n","        co_occurence_matrix[tuple_context_main] = 1\n","      \n","      else:\n","        co_occurence_matrix[tuple_main_context] += 1\n","        co_occurence_matrix[tuple_context_main] += 1\n","\n","print( \"Co occurence matrix built successfully..!!\")\n","print( \"Number of words in vocabulary: \"  + str(len(vocabulary)))\n","print( \"Number of entries in co-occurence matrix: \"  + str(len(co_occurence_matrix)))\n","print( \"Saving the co occurence matrix and vocabulary for future reference.\")\n","\n","pickle.dump( vocabulary, open( \"/content/drive/MyDrive/Design_Project_Verselet/pre_processing+vectoristation/word_embedding2/vocabulary\", \"wb\" ) )\n","pickle.dump( co_occurence_matrix, open( \"/content/drive/MyDrive/Design_Project_Verselet/pre_processing+vectoristation/word_embedding2/co_occurence_matrix\", \"wb\" ) )\n","\n","print(\"Executed Successfully\")"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Global variables initialized...!!\n","Co occurence matrix build in progress... 5000 poems read.\n","Co occurence matrix build in progress... 10000 poems read.\n","Co occurence matrix build in progress... 15000 poems read.\n","Co occurence matrix build in progress... 20000 poems read.\n","Co occurence matrix build in progress... 25000 poems read.\n","Co occurence matrix build in progress... 30000 poems read.\n","Co occurence matrix build in progress... 35000 poems read.\n","Co occurence matrix build in progress... 40000 poems read.\n","Co occurence matrix build in progress... 45000 poems read.\n","Co occurence matrix built successfully..!!\n","Number of words in vocabulary: 24672\n","Number of entries in co-occurence matrix: 6619702\n","Saving the co occurence matrix and vocabulary for future reference.\n","Executed Successfully\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ho9l1p3WWkdA","executionInfo":{"elapsed":428,"status":"ok","timestamp":1635962727681,"user":{"displayName":"201951171 VISHAL SINGH RAJPUT","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11556631215676760612"},"user_tz":-330},"outputId":"6ce28571-25d3-4961-ea3f-74bcec24796e"},"source":["import random\n","import numpy as np\n","vectors_main_word = {}\n","vectors_context_word = {}\n","biases_main_word = {}\n","biases_context_word = {}\n","alpha_glove_model = 0.75\n","x_max_glove_model = 100\n","\n","number_of_iterations = 100\n","learning_rate = 0.001\n","\n","# Method to find the weight as implemented in the research paper\n","def find_weight( main_token, context_token ):\n","  if (context_token,main_token) not in co_occurence_matrix:\n","    return 0\n","  if ( co_occurence_matrix[(context_token,main_token)] < x_max_glove_model ):\n","    return (co_occurence_matrix[(context_token,main_token)] / x_max_glove_model) ** alpha_glove_model\n","  return 1\n","\n","# Randomly initializing the global vectors for all tokens in the vocabulary\n","def initilize_word_vectors_and_biases():\n","  for token in vocabulary:\n","    vectors_main_word[token] = np.random.random(100)\n","    vectors_context_word[token] = np.random.random(100)\n","    biases_main_word[token] = random.random()\n","    biases_context_word[token] = random.random()\n","\n","initilize_word_vectors_and_biases()\n","\n","print(\"Initialization of the word vectors and biases for the \" + str(len(vocabulary)) + \" tokens in our vocabulary complete.\")"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Initialization of the word vectors and biases for the 24672 tokens in our vocabulary complete.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ey16ytiSWlor","executionInfo":{"elapsed":438,"status":"ok","timestamp":1635962735330,"user":{"displayName":"201951171 VISHAL SINGH RAJPUT","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11556631215676760612"},"user_tz":-330},"outputId":"9143d2be-0825-4363-c049-3169b32a58c7"},"source":["# Using alpha and x_max as used by the authors \n","alpha_glove_model = 0.75\n","x_max_glove_model = 100\n","\n","import numpy as np\n","import math\n","\n","# Method to carry out a single iteration of gradient descent using the vectors\n","# More formally, we find the value of the cost and then the gradient and find the new value\n","# by subtracting the learning rate * cost to get the updated value\n","# Implementation NOTE: We have applying gradient descent by batch methods. ie. we find the cost for the entire training set and then apply gradient descent\n","def run_single_iteration():\n","  total_cost = 0\n","  for (context_token,main_token), value in co_occurence_matrix.items():\n","    if main_token == context_token:\n","      continue\n","    weight = find_weight( main_token, context_token )\n","    \n","    if(weight == 0):\n","      continue\n","    cost_without_weight = ( np.dot(vectors_main_word[main_token] , vectors_context_word[context_token] ) + biases_main_word[main_token] + biases_context_word[context_token] - math.log(co_occurence_matrix[(context_token,main_token)]))\n","    total_cost += 0.5 * weight * cost_without_weight ** 2\n","    gradient_main_word_vector = weight * cost_without_weight * vectors_context_word[context_token]\n","    gradient_context_word_vector = weight * cost_without_weight * vectors_main_word[main_token]\n","    gradient_main_bias = weight * cost_without_weight\n","    gradient_context_bias = weight * cost_without_weight\n","\n","    vectors_main_word[ main_token ] -= learning_rate * gradient_main_word_vector\n","    vectors_context_word[ context_token ] -= learning_rate * gradient_context_word_vector\n","\n","    biases_main_word[ main_token ] -= learning_rate * gradient_main_bias\n","    biases_context_word[ context_token ] -= learning_rate * gradient_context_bias\n","  return total_cost\n","\n","print(\"Function to run single iteration of gradient descent compiled successfully..!!\")"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Function to run single iteration of gradient descent compiled successfully..!!\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"bjjvBt-XWpjW","executionInfo":{"status":"error","timestamp":1636005041145,"user_tz":-330,"elapsed":1247,"user":{"displayName":"201951171 VISHAL SINGH RAJPUT","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11556631215676760612"}},"outputId":"1044843e-071a-4236-bb1c-702c10d9c8a9"},"source":["import pickle\n","\n","learning_rate = 0.01\n","print(\"Applying gradient descent to find the appropriate word vectors to carry out unsupervised learning...\")\n","\n","# Driver function that calls the above function and carries out single iteration \n","for iteration in range(1,201):\n","  cost = run_single_iteration()\n","  print(\"Iteration \" + str(iteration) + \" successfull. Returned cost value is: \" + str(cost))\n","\n","print(\"All iterations fot gradient descent completed successfully..!!\")\n","print(\"Saving word vectors in file 'word_vectors' \")\n","pickle.dump(vectors_main_word , open( \"/content/drive/MyDrive/Design_Project_Verselet/pre_processing+vectoristation/word_embedding/word_vectors_test_set_200_iterations\", \"wb\" ) )"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Applying gradient descent to find the appropriate word vectors to carry out unsupervised learning...\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-db5e7972d29e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Driver function that calls the above function and carries out single iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m201\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_single_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" successfull. Returned cost value is: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'run_single_iteration' is not defined"]}]}]}